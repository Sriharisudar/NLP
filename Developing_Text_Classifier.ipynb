{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Developing Text Classifier (Removing Correlated Features)**"
      ],
      "metadata": {
        "id": "yk4KuijIfCII"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9Gak5yCetb9",
        "outputId": "6ebd2e43-f35a-4407-e339-73688ec4f709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "#import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import scipy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "from pylab import *\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**remove the stopwords**"
      ],
      "metadata": {
        "id": "nbt_VLr-g3-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remove the stopwords\n",
        "stop_words=stopwords.words('english')\n",
        "stop_words=stop_words+list(string.printable)\n",
        "lemmatizer=WordNetLemmatizer()\n"
      ],
      "metadata": {
        "id": "XqK7e0JxfHES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**specify the categories of the article**"
      ],
      "metadata": {
        "id": "mrsYxOGog6iO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#specify the categories of the article\n",
        "categories=['misc.forsale','sci.electronics','talk.religion.misc']\n"
      ],
      "metadata": {
        "id": "Qbu43Wq2fJOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**News data and corresponding categories in a Pandas dataframe**"
      ],
      "metadata": {
        "id": "xIo2dHgng9nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#News data and corresponding categories in a Pandas dataframe\n",
        "news_data=fetch_20newsgroups(subset='train',categories=categories,shuffle=True,random_state=42,download_if_missing=True)\n",
        "news_data_df=pd.DataFrame({'text':news_data['data'],'category':news_data.target})\n",
        "news_data_df.head()\n"
      ],
      "metadata": {
        "id": "NC2UVJNMfO3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "fe038cd6-3151-41b1-ad4e-b04daf69d7ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: Steve@Busop.cit.wayne.edu (Steve Teolis)...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>From: jks2x@holmes.acc.Virginia.EDU (Jason K. ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>From: wayne@uva386.schools.virginia.edu (Tony ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>From: lihan@ccwf.cc.utexas.edu (Bruce G. Bostw...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>From: myoakam@cis.ohio-state.edu (micah r yoak...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  category\n",
              "0  From: Steve@Busop.cit.wayne.edu (Steve Teolis)...         0\n",
              "1  From: jks2x@holmes.acc.Virginia.EDU (Jason K. ...         0\n",
              "2  From: wayne@uva386.schools.virginia.edu (Tony ...         1\n",
              "3  From: lihan@ccwf.cc.utexas.edu (Bruce G. Bostw...         1\n",
              "4  From: myoakam@cis.ohio-state.edu (micah r yoak...         0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cleaning the text such as tokenization, lemmatization etc**"
      ],
      "metadata": {
        "id": "0-Zaj9vCg_VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning the text such as tokenization, lemmatization etc\n",
        "news_data_df['cleaned_text']=news_data_df['text'].apply(lambda x :''.join([lemmatizer.lemmatize(word.lower()) for word in word_tokenize(re.sub(r'([^\\s\\w]|_)+','',str(x))) if word.lower() not in stop_words]))\n"
      ],
      "metadata": {
        "id": "NUNHA-M5fRXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**create a TF-IDF matrix**"
      ],
      "metadata": {
        "id": "aYgz0Q0IhBoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a TF-IDF matrix\n",
        "tfidf_model=TfidfVectorizer(max_features=20)\n",
        "tfidf_df=pd.DataFrame(tfidf_model.fit_transform(news_data_df['cleaned_text']).todense())\n",
        "tfidf_df.columns=sorted(tfidf_model.vocabulary_)\n",
        "tfidf_df.head()\n"
      ],
      "metadata": {
        "id": "_MY58RmufWWK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "68ffd240-e1b6-47be-b8cd-38bd1ab96afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>02106raveludeledusamuelrosssubjectbooksalecheapnntppostinghostraveludeleduorganizationuniversitydelawaredistributionusaline28someonepleasebuybookaskingmuchmakeofferprobablytakecalculusanalyticgeometryauthursimoncopyrightdate1982avgconditionstillreadablewritinggoodsoftwarefortrangrahamsmithholthandbookkirsznermandellcopyright1986720pagewritingguidealgebratrigonometryproblemsolvingapproach3rdeditionflemmingvarberggoodconditiongeneralchemistryprinciplemodernapplicationpetruccifourtheditionbigbookgoodconditionsolutionmanualchemistrybookpaperbackstudyguidechemistrybookpaperbacksendofferviaemail02106chopinudeledusam02106chopinudeledu</th>\n",
              "      <th>er1eridanchuvashiasuyarabayevaalbinanikolayevnasubjectsalehighgualityconiferoilrussia450ton400tonreplytoer1eridanchuvashiasudistributioneunetorganizationfirmeridanlineinguiryaddresser1eridanchuvashiasu</th>\n",
              "      <th>jlleeacsubuffaloedujohnnyleesubjectmovingsalesummarymovingsaleorganizationubline44nntppostinghostlictoracsubuffaloedureducedpricelistthingforsalebehalfbrotherwhomovingmovedalreadyofferblackdeckerdusterplusportablehandvaccumpurchased3212sr1000dualcassetteportableplayeramfm5bandgraphicequalizerhighspeeddubingduotapetapedeckseemslosttreblesoundbetfixablepurchased80253monoluxzoommicroscope1200xmagnificationmadejapanincludescaseaccessorypurchased50204sunbeam1400hairdryerdryerputheadunderintoknowoneseesalondontaskbropurchased60245everylastspeedbagleatherbrandnewneverused106osterizerpuslematicblender10speedcookbookyearold10purchased508binoluxbinoculars7x35extrawideangle525ft1000ydscasenew209proctorsilexspraysteamdryironnew10questioncontactthruemailreplyexpeditouslyalwaysshincludedpleaseconsiderlastlyimreasonableveryreasonablethanksjohn</th>\n",
              "      <th>paynecrldeccomandrewpaynesubjectwantedtcm3105chipsmallquantityorganizationdeccambridgeresearchlabline16anyoneknowsourcetcm3105modemchipusedbaycompmpmodemideallysomethinggearedtowardhobbyistsmallquantitymailorderetcyearwevebuyingdistributormarshallhundredpmpkitorderdroppedpointlongeraffordofferservicedistributorivecheckedcrazyminimumorder100idlikefindsourcestillinterestedbuildingpmpkitsuggestionandrewpaynedeccambridgeresearchlab</th>\n",
              "      <th>pchangicsunysbedupongchangsubjectcomputerpartscamcordernntppostinghostlibws4icsunysbeduorganizationstateuniversitynewyorkstonybrookline21articlec4z2cf2n6cscnscomzardoxcscnscomrandieonealwrites52512mb35144drivesnewplannedbuildingmachineranfund3000drivecarlbranddrivedocumentationillgo4000shippingdocumentationpay40floppydrive40newalsoneeddocumentationfloppyinstallationthingidiotproofadvicecommonpchangicsunysbedusensestateuniversitynewyorkstonybrookengineer</th>\n",
              "      <th>pchangicsunysbedupongchangsubjectmicrosoftdo60upgradesalenntppostinghostlibws4icsunysbeduorganizationstateuniversitynewyorkstonybrookline36articlehatton733706165cglucsfeduhattonsocratesucsfedutomhattonwritesadn6285ritvaxiscriteduwritesarticle1pctnfinn6dpeveusceduyuanchieeveusceduyuanchiehhsuwritesmdo60upgradesalebestoffer45openedunregisteredanyonecareenlightenuwhetherdos60worthupgradinggoodcompressionturnedonoffnicenastyfeatureaccordingreportdontdoyetdontutilityqemmstackerpctoolsnortondos6mayworthpeopledos5sortutilitydos6doesntoffermuchyoudneverknowusualhypemarketingablecreatehoweverinstalleddolastweeknothingtroubleafterwardswindowappshittingprotectionfaultkidpinataseemlikeworkndosnortondo70probsincludesetpcplusdpcplusprocommpluslongerworkmanylittleutilitywrittendolongerworkeithermostlysharewareuninstalleddodoworkfineappsdoablerundowontcommonpchangicsunysbedusensestateuniversitynewyorkstonybrookengineer</th>\n",
              "      <th>pchangicsunysbedupongchangsubjectsalec128systemwprinter130obonntppostinghostlibws4icsunysbeduorganizationstateuniversitynewyorkstonybrookline20commodore128epsonhomewriter10pinprinter1571ddiskdrivejoystickmouselotsasoftwaregameappsrapidfirejoystickadapteryearold130obocommonpchangicsunysbedusensestateuniversitynewyorkstonybrookengineer</th>\n",
              "      <th>pchangicsunysbedupongchangsubjectsalequicken30pckeywordsaccountingcheckingquickennntppostinghostlibws4icsunysbeduorganizationstateuniversitynewyorkstonybrookline18article1pma84hpksuntanecusfedubonehamsunburnecusfedukevinbonehamchwritesarticle1pgvp1inn5ejphaktuscedukhohusceduolivermuotowritessalequicken30pcversionallowsbalanceassumegetnewreleaseearlierlastsawversion20latestprobablyreferringdoversiondoversionlikeversionthinkwindowversioncamerecentlylikeversionsomethingcommonpchangicsunysbedusensestateuniversitynewyorkstonybrookengineer</th>\n",
              "      <th>pchangicsunysbedupongchangsubjectvideotitlemakersalenntppostinghostlibws4icsunysbeduorganizationstateuniversitynewyorkstonybrookline37videonicstitlemakersystemmonthesoldusedincludescharactergeneratormodeltm1rez720x4808000availablechar12fontstereosoundmilliondifferentcoloravailable20specialeffectfullkeyboarddesignmaildetailthumsvideoeditormodeltu1markdiffernentsectiontapethumbthumbskipbadparttapebuiltinvideoenhancercopyingtapeviewingautomaticfaderswitchableusecombinationunitunitexcellentconditioncomedocunregisteredwarrantycardjrmusicworldsell399229respectivelyasking500unitemailpchangicsunysbeduinterestedcommonpchangicsunysbedusensestateuniversitynewyorkstonybrookengineer</th>\n",
              "      <th>pcwoodastroocistempleedupaulwoodsubjectforsalegenesisgameorganizationtempleuniversityline20nntppostinghostastroocistempleeduxnewsreadertinversion11pl8kelvinwilliamskwilllunatixuucpwrotegameforsaletradesonichedgehogiitwocopymanualcase25brandnewhellointerestedsoniciisendaddressgetmailbouncedbackhostunknowerrorpleasereplykwilllunatixuucpsubjectgamepaulwoodpcwoodastroocistempleedu</th>\n",
              "      <th>peavlerfingalplkafmilninjagourmetsubjectscarlethorsebabylondaemonorganizationuniversitynewmexicoalbuquerquenmline20distributionworldnntppostinghostfingalplkafmilkeywordsdeadhorsehorsebabylonarticle1qilgninnrkolynxunmedublowfishleounmeduronwriteseasy667neighborbeast666beastlifeendculdesacnoticeddeadhorsekeywordslinefamousscarlethorsebabylonbeastthats666illuminattiridewonderfulmediaevalmanuscriptfearannouncementoldgirldeadmayprematurebet20place6thracedownlastsundayslidbadfifthdeadcomatoselikegodwaysupposeninjagourmetfightfoodjimpeavleropinionexistpeavlerplkafmilcalledalbuquerquenmopinion</th>\n",
              "      <th>pegasusaaauoregonedupegasussubjectmerlinmithramagickorganizationpolyhedrongroupline21distributionworldnntppostinghostfp1dialin1uoregoneduarticlejoshua93apr19183833baileycpacwashingtonedujoshuacpacwashingtonedujoshuagellerwrotearticlepegasus150493132018fp1dialin4uoregonedupegasusaaauoregonedulaurieewbrandtwriteslewbletaddpercentage1315orphaicdocteriansbroughtlewbtogrouppaulsaulhighrankinginitiatelewbdevelopmentorphaicmysteryseejaneharrisonprolegomenonlewbstudygreekreligioncambridgepress1922easlydrawlewbyourconclusionjoshperhapsquotebitargumentlovetobutmustbitlatercopyharrisonpackedlastchapterbestremberdealorphicmysteryviewwomanthoughcomesaystronglyimplyedchristianviewdrawnheavlyorphicmajorculttimepegasus</th>\n",
              "      <th>pepkediracscrifsueduericpepkesubjectsocietalbasismoralityorganizationfloridastateuniversitydontspeakline13articlemerlyn735422443digibdmerlyndigibddigibdcommerlynleroywritesprayerschoollegalillegaltellingchildpraypraymanypeopleconfusecanttellkidoughtpraykidarentallowedpraypossiblykidwithouttoldperhapsthinkgovernmentalbodybusinesssuppressingbeliefelsetheyresortsatanichumanistconspiracyoldyoureuyoureubitemp</th>\n",
              "      <th>peterminsaneapanaorgaupetertryndochsubjectdmmadviceneededline28allmartinemdedmmadviceneededmefrommce5921bcsteccaboeingcommartinemdemeorganizationboeingmeicurrelymarketdmmrecentlysawaddmeforkelvin94199anyoneonemeotherbrandextremelyhappymesmallnamebrandcompareflukebeckmanbrandmeiwillingspend200onemeanyhelpgreatlyappreciatedpleaseemailmemartingoinguseonecountegaviationspacescuttleetcsuggestgobuyflukeneverseenbeckmanhowevereveryusebuycheapiemetexmadenameseendmmbrandnameboughtyragoaus12500convertuseedefinetlycheapiefarprovedaccuratetakenmoderateabusemanyfeaturecapfreqtransistorchecketchappywoulddefinetlybuyflukenamehopehelpcheerpeter</th>\n",
              "      <th>peterminsaneapanaorgaupetertryndochsubjectswrmetercbradioline28allthedevilreincarnateswrmetercbradiotdfromssaveolecdaccomdevilreincarnatetdorganizationcdacwatdwhattdisgoodchoicecb1418wavetdreadinstallationinstruction14waveantennatdandsuggesteduseswrtunechannel12tdandchannel32minimumreadingquestionchanneltd1232bestantennaoneletwaveprobablybestexplanationrestmakesenseonewavecancellbtwbeastiebest12waveantennafollowed1418etcswringactuallytrimantennacorrectlengthspecificwavelengthtransmittingsincewavelengthvarieschanneluserecommendedswrusingmiddlechannelgoinguseanywaybeginningcbsnewantennaswrednowdaysmanufacturetrimantennaalmostspottheremuchpointswringmayfanaticwhishanywaycheerpeter</th>\n",
              "      <th>peterminsaneapanaorgaupetertryndochsubjecttelephonehookoffhokline17allmichaelcovingtontelephonehookoffhokmcfrommcovingtaisun3aiugaedumichaelcovingtonmcorganizationaiprogramuniversitygeorgiaathensmcayethererubdrawenoughcurrentlightledthmcequipmentphonecompanythinkyouvegonehookmcinonhookstateyouresupposeddrawcurrentokletcalculationgoingaustralianstandardpresumemightsimilarcountryletinputphoneus600ohmloop48vline80mastandardleddrain20maactualloopcurrentrequiredhookindicationknowcheerpeter</th>\n",
              "      <th>peterminsaneapanaorgaupetertryndochsubjectwhatsexactlyflourline32allmartinmccormickwhatsexactlyflourmmfrommartindatacommuccokstateedumartinmccormickmmorganizationoklahomastateuniversitystillwaterokmmsortlamplittleglassbulbfoundinsidestartermmitsortremindsne2neonlampstarterappearopenmmwithohmmeterlittlelampeitherneoncapacitmminseriesmmmmseenthinglifeneverreadreallygoodmmdescriptionhappeninginsidelittleknowbimetallicstripcasetwostripdifferentmetalbondedtogetherheatedbendonesidecheckblinkerglobechristmastreelightturnpowercausebulbworklikeneonheatingshortingthusprovidinglooppowerheatermaintubetubefireinsufficientcurrentrunstarterkeepheatbimetalicstripstraightensocbtwthoughtnothingsmallneononedayneonsisterdigitalflipmetalsquaretypeclockbrokeflimsyleadreplacedonestarterwellpoweringmadebitmessclockcheerpeter</th>\n",
              "      <th>peterminsaneapanaorgaupetertryndochsubjectwhatsgoodicrs23line13alltallcoolonewhatsgoodicrs23tcfromrky57514uxacsouiucedutallcoolonetcorganizationuniversityillinoisurbanatcimlookingicconvertrs232voltagelevelttlvotclevelssomethingrelativelyinexpensivewouldniceanyonetcasuggestionthankstrymaximmax232cpepindilconvert5v12v232commmscleverlittlegizmopeter</th>\n",
              "      <th>petertoddchanpc1oandrewcmuedusubjectklipschfortespkrssaleorganizationfifthyrseniorelectricalcomputerengineeringcarnegiemellonpittsburghpaline14nntppostinghostpo3andrewcmueduitemklipschfortespeakerconditionmintagemontholdprice1000pairretail1400pairspeakerperfectconditionusedaudiophilesystemfloorstandingcomeoriginalpackagaingliteraturealsostillwarrantyinterestedquestionpleasefeelfreeemailpc1oandrewcmueducallhomethanksjon4128826425</th>\n",
              "      <th>petertoddchanpc1oandrewcmuedusubjectreducedsonycdplayersaleorganizationfifthyrseniorelectricalcomputerengineeringcarnegiemellonpittsburghpaline21nntppostinghostpo5andrewcmueduitemsonyescdpx229conditionexcellentageyearoldprice300includestoslinkitemsonycdp770conditionexcellentage25yearoldprice250everythingcomeoriginalpackagingmanualitemplayedaudiophilesystemexcellentshapeinterestedneedadditionalinformationpleaseemailpc1oandrewcmueducallhomethanksjon4128826425pyessale</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   02106raveludeledusamuelrosssubjectbooksalecheapnntppostinghostraveludeleduorganizationuniversitydelawaredistributionusaline28someonepleasebuybookaskingmuchmakeofferprobablytakecalculusanalyticgeometryauthursimoncopyrightdate1982avgconditionstillreadablewritinggoodsoftwarefortrangrahamsmithholthandbookkirsznermandellcopyright1986720pagewritingguidealgebratrigonometryproblemsolvingapproach3rdeditionflemmingvarberggoodconditiongeneralchemistryprinciplemodernapplicationpetruccifourtheditionbigbookgoodconditionsolutionmanualchemistrybookpaperbackstudyguidechemistrybookpaperbacksendofferviaemail02106chopinudeledusam02106chopinudeledu  ...  petertoddchanpc1oandrewcmuedusubjectreducedsonycdplayersaleorganizationfifthyrseniorelectricalcomputerengineeringcarnegiemellonpittsburghpaline21nntppostinghostpo5andrewcmueduitemsonyescdpx229conditionexcellentageyearoldprice300includestoslinkitemsonycdp770conditionexcellentage25yearoldprice250everythingcomeoriginalpackagingmanualitemplayedaudiophilesystemexcellentshapeinterestedneedadditionalinformationpleaseemailpc1oandrewcmueducallhomethanksjon4128826425pyessale\n",
              "0                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ...                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
              "1                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ...                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
              "2                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ...                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
              "3                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ...                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
              "4                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ...                                                0.0                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**calculate the correlation matrix for TF-IDF matrix**"
      ],
      "metadata": {
        "id": "YamWGVv7hDnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the correlation matrix for TF-IDF matrix\n",
        "correlation_matrix=tfidf_df.corr()\n",
        "correlation_matrix.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "oETLzt3XfYCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**plot the correlation matrix using seaborn’s heatmap function**"
      ],
      "metadata": {
        "id": "RxhhUDPkhGDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plot the correlation matrix using seaborn’s heatmap function\n",
        "fig, ax=plt.subplots(figsize=(20,20))\n",
        "sns.heatmap(correlation_matrix,annot=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1XeGHkm2faYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**identify the pair of terms with high correlation, we created an upper triangular matrix from the correlation matrix.**"
      ],
      "metadata": {
        "id": "r4jQixtKhIq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#identify the pair of terms with high correlation, we created an upper triangular matrix from the correlation matrix.\n",
        "correlation_matrix_ut=correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape)).astype(np.bool))\n",
        "correlation_matrix_melted=correlation_matrix_ut.stack().reset_index()\n",
        "correlation_matrix_melted.columns=['word1','word2','correlation']\n",
        "correlation_matrix_melted[(correlation_matrix_melted['word1']!=correlation_matrix_melted['word2'])&(correlation_matrix_melted['correlation']>.7)]\n"
      ],
      "metadata": {
        "id": "GJXVZacZfi7P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "89fd29ba-732e-4e67-aa7a-6d9467f63337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word1</th>\n",
              "      <th>word2</th>\n",
              "      <th>correlation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [word1, word2, correlation]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**we will remove terms for which the coefficient of correlation is >0.7 and create a separate Dataframe with the remaining terms.**"
      ],
      "metadata": {
        "id": "Z4EjR4vQhKwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we will remove terms for which the coefficient of correlation is >0.7 and create a separate Dataframe with the remaining terms.\n",
        "tfidf_tf_without_correlated_word=tfidf_df.drop(['02'],axis=1)\n",
        "tfidf_tf_without_correlated_word.head()\n"
      ],
      "metadata": {
        "id": "1dbhxyvNfkfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Developing Text Classifier (Dimensionality Reduction using PCA)**"
      ],
      "metadata": {
        "id": "pMBNB0gLfvTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import scipy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "from pylab import *\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "TRbZwu8sfsEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**remove the stopwords**"
      ],
      "metadata": {
        "id": "DJLMMkkkhNhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remove the stopwords\n",
        "stop_words=stopwords.words('english')\n",
        "stop_words=stop_words+list(string.printable)\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "stop_words"
      ],
      "metadata": {
        "id": "5as20oHtf01N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**specify the categories of the article**"
      ],
      "metadata": {
        "id": "ZVAGilNMhPeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#specify the categories of the article\n",
        "categories=['misc.forsale','sci.electronics','talk.religion.misc']\n"
      ],
      "metadata": {
        "id": "RhgyQIssf43X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**News data and corresponding categories in a Pandas dataframe**"
      ],
      "metadata": {
        "id": "fU99ob4KhRhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#News data and corresponding categories in a Pandas dataframe\n",
        "news_data=fetch_20newsgroups(subset='train',categories=categories,shuffle=True,random_state=42,download_if_missing=True)\n",
        "news_data_df=pd.DataFrame({'text':news_data['data'],'category':news_data.target})\n",
        "news_data_df.head()\n"
      ],
      "metadata": {
        "id": "LxDDDe5Rf6QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cleaning the text such as tokenization, lemmatization etc**"
      ],
      "metadata": {
        "id": "YZw8p_MqhTyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning the text such as tokenization, lemmatization etc\n",
        "news_data_df['cleaned_text']=news_data_df['text'].apply(lambda x :''.join([lemmatizer.lemmatize(word.lower()) for word in word_tokenize(re.sub(r'([^\\s\\w]|_)+','',str(x))) if word.lower() not in stop_words]))\n",
        "news_data_df['cleaned_text']\n"
      ],
      "metadata": {
        "id": "9jzJwEj2f7yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**create a TF-IDF matrix**"
      ],
      "metadata": {
        "id": "eR4N-dYmhVpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a TF-IDF matrix\n",
        "tfidf_model=TfidfVectorizer(max_features=20)\n",
        "tfidf_df=pd.DataFrame(tfidf_model.fit_transform(news_data_df['cleaned_text']).todense())\n",
        "tfidf_df.columns=sorted(tfidf_model.vocabulary_)\n",
        "tfidf_df.head()\n"
      ],
      "metadata": {
        "id": "TmHSOF3BgAfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**sklearn’s PCA function to extract two principal components from the earlier data.**"
      ],
      "metadata": {
        "id": "-too_mRDhYSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sklearn’s PCA function to extract two principal components from the earlier data.\n",
        "from sklearn.decomposition import PCA\n",
        "pca=PCA(2)\n",
        "pca.fit(tfidf_df)\n",
        "reduced_tfidf=pca.transform(tfidf_df)\n",
        "reduced_tfidf\n"
      ],
      "metadata": {
        "id": "D5ou3gK5gDsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**create a scatter plot along these principal components and represent each category with a separate color.**"
      ],
      "metadata": {
        "id": "kSNarg1LhbRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a scatter plot along these principal components and represent each category with a separate color.\n",
        "plt.scatter(reduced_tfidf[:,0],reduced_tfidf[:,1],c=news_data_df['category'],cmap='viridis')\n",
        "plt.xlabel('dimension_1')\n",
        "plt.ylabel('dimension_2')\n",
        "plt.title('Representation of NEWS documents in 2D')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T81Wm12hgGbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving and Loading Models**"
      ],
      "metadata": {
        "id": "hQPbylBkgKiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving and Loading Models\n",
        "\n",
        "#import necessary packages\n",
        "\n",
        "import pickle\n",
        "from joblib import dump, load\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "metadata": {
        "id": "UdGZRPUzgLdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**defining a corpus consisting of four sentences**"
      ],
      "metadata": {
        "id": "1hvMbta4hfqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining a corpus consisting of four sentences\n",
        "corpus = ['Data Science is an overlap between Arts and Science', 'Generally, Arts graduates are right brained and Science graduates are left-brained','Excelling in both Arts and Science at a time becomes difficult','Natural Language Processing is a part of Data Science']\n"
      ],
      "metadata": {
        "id": "aa3OSrCdgO9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**fit a tf-idf model**"
      ],
      "metadata": {
        "id": "fpJu1LMihii9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fit a tf-idf model\n",
        "tfidf_model=TfidfVectorizer()\n",
        "print(tfidf_model.fit_transform(corpus).todense())\n"
      ],
      "metadata": {
        "id": "r9ZNaYUpgRRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**save this tf-idf model on disk using joblib.**"
      ],
      "metadata": {
        "id": "7k8aCQShhmRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save this tf-idf model on disk using joblib.\n",
        "dump(tfidf_model,'tfidf_model.joblib')\n"
      ],
      "metadata": {
        "id": "E5o74IxPgU6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513e80ff-79cb-4a0f-c411-5a164ae89112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**load this model from the disk to the memory and use it**"
      ],
      "metadata": {
        "id": "VsRhvZiGho9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load this model from the disk to the memory and use it\n",
        "tfidf_model_loaded=load('tfidf_model.joblib')\n",
        "print(tfidf_model_loaded.fit_transform(corpus).todense())\n"
      ],
      "metadata": {
        "id": "sxaXJLT7gYR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**save this tf-idf model on disk using pickle**"
      ],
      "metadata": {
        "id": "lA6fwi4Ohrce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save this tf-idf model on disk using pickle\n",
        "pickle.dump(tfidf_model,open(\"tfidf_model.pickle.dat\",\"wb\"))\n"
      ],
      "metadata": {
        "id": "NYzV-f01gadH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**load this model from the disk to the memory and use it**"
      ],
      "metadata": {
        "id": "uvU45qFUhuoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load this model from the disk to the memory and use it\n",
        "loaded_model=pickle.load(open(\"tfidf_model.pickle.dat\",\"rb\"))\n",
        "\n",
        "print(loaded_model.fit_transform(corpus).todense())\n"
      ],
      "metadata": {
        "id": "giLg7m3sgdYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zeovw2ewgfYE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}